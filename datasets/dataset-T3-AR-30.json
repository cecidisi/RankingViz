{
    "query":"augmented reality",
    "dataset-id":"T3-30",
    "topic" : "T3 AR",
    "description":"Topic 3 - 30 items AR",
    "text":"An augmented reality system generates a composite view for the user that is the combination of the real scene viewed by the user and a virtual scene generated by the computer that augments the scene with additional information. The virtual scene generated by the computer is designed to enhance the user's sensory perception of the virtual world they are seeing or interacting with. ",
    "task": "Select the 5 most relevant recommendations for the following text:",
    "questions":[
        "Virtual environments",
        "Context-based object recognition",
        "Augmented reality (AR) is a live direct or indirect view of a physical, real-world environment whose elements are augmented (or supplemented) by computer-generated sensory input such as sound, video, graphics or GPS data"
    ],
    "totalResults":"30",
    "data":[
        {
            "id":"e42f8109-4a95-34d8-b295-3e81d6d5f683",
            "title":"Taxonomy of mixed reality visual displays",
            "uri":"http://www.mendeley.com/catalog/taxonomy-mixed-reality-visual-displays-2/",
            "eexcessURI":"http://www.mendeley.com/catalog/taxonomy-mixed-reality-visual-displays-2/",
            "creator":"Paul Milgram, Fumio Kishino",
            "description":"This paper focuses on Mixed Reality (MR) visual displays, a particular subset of Virtual Reality (VR) related technologies that involve the merging of real and virtual worlds somewhere along the \"virtuality continuum\" which connects completely real environments to completely virtual ones. Probably the best known of these is Augmented Reality (AR), which refers to all cases in which the display of an otherwise real environment is augmented by means of virtual (computer graphic) objects. The converse case on the virtuality continuum is therefore Augmented Virtuality (AV). Six classes of hybrid MR display environments are identified. However, an attempt to distinguish these classes on the basis of whether they are primarily video or computer graphics based, whether the real world is viewed directly or via some electronic display medium, whether the viewer is intended to feel part of the world or on the outside looking in, and whether or not the scale of the display is intended to map orthoscopically onto the real world leads to quite different groupings among the six identified classes, thereby demonstrating the need for an efficient taxonomy, or classification framework, according to which essential differences can be identified. The 'obvious' distinction between the terms \"real\" and \"virtual\" is shown to have a number of different aspects, depending on whether one is dealing with real or virtual objects, real or virtual images, and direct or non-direct viewing of these. An (approximately) three dimensional taxonomy is proposed, comprising the following dimensions: Extent of World Knowledge (\"how much do we know about the world being displayed?\"), Reproduction Fidelity (\"how 'realistically' are we able to display it?\"), and Extent of Presence Metaphor (\"what is the extent of the illusion that the observer is present within that world?\"). key words: virtual reality (VR), augmented reality (AR), mixed reality (MR)",
            "collectionName":"IEICE Transactions on Information and Systems",
            "facets":{
                "provider":"mendeley",
                "year":"1994"
            }
        },
        {
            "id":"0ebacefb-ef02-3ebe-9a6a-19444794391a",
            "title":"Augmented reality and photogrammetry: A synergy to visualize physical and virtual city environments",
            "uri":"http://www.mendeley.com/catalog/augmented-reality-photogrammetry-synergy-visualize-physical-virtual-city-environments/",
            "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-photogrammetry-synergy-visualize-physical-virtual-city-environments/",
            "creator":"Cristina Portal&amp;eacute;s, Jos&amp;eacute; Luis Lerma, Santiago Navarro",
            "description":"Close-range photogrammetry is based on the acquisition of imagery to make accurate measurements and, eventually, three-dimensional (3D) photo-realistic models. These models are a photogrammetric product per se. They are usually integrated into virtual reality scenarios where additional data such as sound, text or video can be introduced, leading to multimedia virtual environments. These environments allow users both to navigate and interact on different platforms such as desktop PCs, laptops and small hand-held devices (mobile phones or PDAs). In very recent years, a new technology derived from virtual reality has emerged: Augmented Reality (AR), which is based on mixing real and virtual environments to boost human interactions and real-life navigations. The synergy of AR and photogrammetry opens up new possibilities in the field of 3D data visualization, navigation and interaction far beyond the traditional static navigation and interaction in front of a computer screen. In this paper we introduce a low-cost outdoor mobile AR application to integrate buildings of different urban spaces. High-accuracy 3D photo-models derived from close-range photogrammetry are integrated in real (physical) urban worlds. The augmented environment that is presented herein requires for visualization a see-through video head mounted display (HMD), whereas user's movement navigation is achieved in the real world with the help of an inertial navigation sensor. After introducing the basics of AR technology, the paper will deal with real-time orientation and tracking in combined physical and virtual city environments, merging close-range photogrammetry and AR. There are, however, some software and complex issues, which are discussed in the paper. &amp;copy; 2009 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).",
            "collectionName":"ISPRS Journal of Photogrammetry and Remote Sensing",
            "facets":{
                "provider":"mendeley",
                "year":"2010"
            }
        },
        {
            "id":"fd2779b4-12d1-3dbc-8584-ba04befa0089",
            "title":"Augmented Reality: An Overview",
            "uri":"http://www.mendeley.com/catalog/augmented-reality-overview/",
            "eexcessURI":"http://www.mendeley.com/catalog/augmented-reality-overview/",
            "creator":"Julie Carmigniani, Borko Furht",
            "description":"We define Augmented Reality (AR) as a real-time direct or indirect view of a physical real-world environment that has been enhanced/ augmented by adding virtual computer-generated information to it 1. AR is both interactive and registered in 3D as well as combines real and virtual objects. Milgrams Reality-Virtuality Continuum is defined by Paul Milgram and Fumio Kishino as a continuum that spans between the real environment and the virtual environment comprise Augmented Reality and Augmented Virtuality (AV) in between, where AR is closer to the real world and AV is closer to a pure virtual environment, as seen in Fig. 1.1 2.",
            "collectionName":"Handbook of Augmented Reality",
            "facets":{
                "provider":"mendeley",
                "year":"2011"
            }
        },
        {
            "id":"d8b63226-3000-3e04-afdd-f2a31a3da6e9",
            "title":"Puzzle assembly training: Real world vs virtual environment",
            "uri":"http://www.mendeley.com/catalog/puzzle-assembly-training-real-world-vs-virtual-environment/",
            "eexcessURI":"http://www.mendeley.com/catalog/puzzle-assembly-training-real-world-vs-virtual-environment/",
            "creator":"Mike Oren, Patrick Carlson, Stephen Gilbert, Judy M. Vance",
            "description":"While training participants to assemble a 3D wooden burr puzzle, we compared results of training in a stereoscopic, head tracked virtual assembly environment utilizing haptic devices and data gloves with real world training. While virtual training took participants about three times longer, the group that used the virtual environment was able to assemble the physical test puzzle about three times faster than the group trained with the physical puzzle. We present several possible cognitive explanations for these results and our plans for future exploration of the factors that improve the effectiveness of virtual process training over real world experience.",
            "collectionName":"Proceedings - IEEE Virtual Reality",
            "facets":{
                "provider":"mendeley",
                "year":"2012"
            }
        },
        {
            "id":"b1477783-aeed-3101-84bc-2b98024e1929",
            "title":"An augmented virtuality approach to 3D videoconferencing",
            "uri":"http://www.mendeley.com/catalog/augmented-virtuality-approach-3d-videoconferencing/",
            "eexcessURI":"http://www.mendeley.com/catalog/augmented-virtuality-approach-3d-videoconferencing/",
            "creator":"H. Regenbrecht, C. Ott, M. Wagner, T. Lum, P. Kohler, W. Wilke, E. Mueller",
            "description":" This paper describes the concept, prototypical implementation, and usability evaluation of an augmented virtuality (AV) based videoconferencing (VC) system: \"cAR/PE!\". We present a first solution which allows three participants at different locations to communicate over a network in an environment simulating a traditional face-to-face meeting. Integrated into the AV environment are live video streams of the participants spatially arranged around a virtual table, a large virtual presentation screen for 2D display and application sharing, and 3D geometry (models) within the room and on top of the table.",
            "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
            "facets":{
                "provider":"mendeley",
                "year":"2003"
            }
        },
        {
            "id":"42a426c2-e926-33d8-b8b8-e1347e2c4bd3",
            "title":"From appearance to context-based recognition: Dense labeling in small images",
            "uri":"http://www.mendeley.com/catalog/appearance-contextbased-recognition-dense-labeling-small-images/",
            "eexcessURI":"http://www.mendeley.com/catalog/appearance-contextbased-recognition-dense-labeling-small-images/",
            "creator":"Devi Parikh, C. Lawrence Zitnick, Tsuhan Chen",
            "description":"Traditionally, object recognition is performed based solely on the appearance of the object. However, relevant information also exists in the scene surrounding the object. As supported by our human studies, this contextual information is necessary for accurate recognition in low resolution images. This scenario with impoverished appearance information, as opposed to using images of higher resolution, provides an appropriate venue for studying the role of context in recognition. In this paper, we explore the role of context for dense scene labeling in small images. Given a segmentation of an image, our algorithm assigns each segment to an object category based on the segmentpsilas appearance and contextual information. We explicitly model context between object categories through the use of relative location and relative scale, in addition to co-occurrence. We perform recognition tests on low and high resolution images, which vary significantly in the amount of appearance information present, using just the object appearance information, the combination of appearance and context, as well as just context without object appearance information (blind recognition). We also perform these tests in human studies and analyze our findings to reveal interesting patterns. With the use of our context model, our algorithm achieves state-of-the-art performance on MSRC and Corel. datasets.",
            "collectionName":"26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "facets":{
                "provider":"mendeley",
                "year":"2008"
            }
        },
        {
            "id":"75c325fc-dd0f-3bfe-a5f8-872eebf8bc9c",
            "title":"Context based object categorization: A critical survey",
            "uri":"http://www.mendeley.com/catalog/context-based-object-categorization-critical-survey-5/",
            "eexcessURI":"http://www.mendeley.com/catalog/context-based-object-categorization-critical-survey-5/",
            "creator":"Carolina Galleguillos, Serge Belongie",
            "description":"The goal of object categorization is to locate and identify instances of an object category within an image. Recognizing an object in an image is difficult when images include occlusion, poor quality, noise or background clutter, and this task becomes even more challenging when many objects are present in the same scene. Several models for object categorization use appearance and context information from objects to improve recognition accuracy. Appearance information, based on visual cues, can successfully identify object classes up to a certain extent. Context information, based on the interaction among objects in the scene or global scene statistics, can help successfully disambiguate appearance inputs in recognition tasks. In this work we address the problem of incorporating different types of contextual information for robust object categorization in computer vision. We review different ways of using contextual information in the field of object categorization, considering the most common levels of extraction of context and the different levels of contextual interactions. We also examine common machine learning models that integrate context information into object recognition frameworks and discuss scalability, optimizations and possible future approaches. ?? 2010 Elsevier Inc.",
            "collectionName":"Computer Vision and Image Understanding",
            "facets":{
                "provider":"mendeley",
                "year":"2010"
            }
        },
        {
            "id":"ea8e2abe-94fe-39ab-875f-4b5c17cdd1ff",
            "title":"Exploring tiny images: The roles of appearance and contextual information for machine and human object recognition",
            "uri":"http://www.mendeley.com/catalog/exploring-tiny-images-roles-appearance-contextual-information-machine-human-object-recognition/",
            "eexcessURI":"http://www.mendeley.com/catalog/exploring-tiny-images-roles-appearance-contextual-information-machine-human-object-recognition/",
            "creator":"Devi Parikh, C. Lawrence Zitnick, Tsuhan Chen",
            "description":"Typically, object recognition is performed based solely on the appearance of the object. However, relevant information also exists in the scene surrounding the object. In this paper, we explore the roles that appearance and contextual information play in object recognition. Through machine experiments and human studies, we show that the importance of contextual information varies with the quality of the appearance information, such as an image's resolution. Our machine experiments explicitly model context between object categories through the use of relative location and relative scale, in addition to co-occurrence. With the use of our context model, our algorithm achieves state-of-the-art performance on the MSRC and Corel data sets. We perform recognition tests for machines and human subjects on low and high resolution images, which vary significantly in the amount of appearance information present, using just the object appearance information, the combination of appearance and context, as well as just context without object appearance information (blind recognition). We also explore the impact of the different sources of context (co-occurrence, relative-location, and relative-scale). We find that the importance of different types of contextual information varies significantly across data sets such as MSRC and PASCAL.",
            "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "facets":{
                "provider":"mendeley",
                "year":"2012"
            }
        },
        {
            "id":"27ef8028-7b52-386a-a98f-abc671ed9756",
            "title":"Context-based vision system for place and object recognition",
            "uri":"http://www.mendeley.com/catalog/contextbased-vision-system-place-object-recognition/",
            "eexcessURI":"http://www.mendeley.com/catalog/contextbased-vision-system-place-object-recognition/",
            "creator":"A. Torralba, K.P. Murphy, W.T. Freeman, M.A. Rubin",
            "description":"While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.",
            "collectionName":"Proceedings Ninth IEEE International Conference on Computer Vision",
            "facets":{
                "provider":"mendeley",
                "year":"2003"
            }
        },
        {
            "id":"e3dd0288-502d-3846-9e7e-cfe477f30d56",
            "title":"The role of context in object recognition",
            "uri":"http://www.mendeley.com/catalog/role-context-object-recognition-16/",
            "eexcessURI":"http://www.mendeley.com/catalog/role-context-object-recognition-16/",
            "creator":"Aude Oliva, Antonio Torralba",
            "description":"In the real world, objects never occur in isolation; they co-vary with other objects and particular environments, providing a rich source of contextual associations to be exploited by the visual system. A natural way of representing the context of an object is in terms of its relationship to other objects. Alternately, recent work has shown that a statistical summary of the scene provides a complementary and effective source of information for contextual inference, which enables humans to quickly guide their attention and eyes to regions of interest in natural scenes. A better understanding of how humans build such scene representations, and of the mechanisms of contextual analysis, will lead to a new generation of computer vision systems. ?? 2007 Elsevier Ltd. All rights reserved.",
            "collectionName":"Trends in Cognitive Sciences",
            "facets":{
                "provider":"mendeley",
                "year":"2007"
            }
        },
        {
            "id":"643c8a34-ee4a-34b4-9a8d-8e2b7c8ace93",
            "title":"Rendering Methods for Augmented Reality",
            "uri":"http://www.mendeley.com/catalog/rendering-methods-augmented-reality/",
            "eexcessURI":"http://www.mendeley.com/catalog/rendering-methods-augmented-reality/",
            "creator":"Jan Fischer",
            "description":"Augmented reality (AR) has become a promising and fast-growing application of computer graphics over the course of the last years. Augmented reality systems overlay computer-generated graphical information over the view of the real world. Several main research challenges can be identified in the field of augmented reality. These are the design of advanced display devices (e.g., head-mounted displays), camera tracking, system design, user interaction, and rendering. While a major part of the previous work focused on the problems of system design, camera tracking, and applications of AR, this thesis puts a different emphasis on the relatively underrepresented aspect of rendering techniques. In this thesis, several novel methods for displaying augmented video streams are explored. In the first part of this thesis in Chapter 2, the design and implementation of a novel system for medical augmented reality are discussed. The ARGUS framework is a new augmented reality system based on a commercial surgical navigation device. Since it does not require any additional hardware components, a transition into the clinical practice can be facilitated. Several extensions of the basic framework are described, including a hybrid tracking scheme, a user interaction library, and amethod for handling occlusion. The latter algorithm makes it possible to correctly render the occlusion of graphical objects by the anatomy of the patient, leading to a more realistic and easily comprehensible output. This approach is one of the advanced rendering methods for augmented reality investigated in the context of this thesis. The second part of this thesis, Chapter 3, introduces the concept of stylized augmented reality, which applies artistic or illustrative stylization methods to augmented reality video streams. Since the same type of stylization is applied to virtual and real scene elements, they become difficult to distinguish. This way, a novel augmented reality experience is created, and possibly even a better immersion. Real-time algorithms for cartoon-like and painterly brush stroke stylization of augmented video streams are described. The exploitation of programmable graphics hardware for this purpose is discussed. Moreover, the results of a psychophysical study on the discernability of virtual objects in stylized augmented reality are presented. At the beginning of Chapter 4, which is the third part of this thesis, a novel illustrative visualization method is described. This new rendering algorithm for iso-surfaces and polygonal models generates an illustrative representation of a surface and structures hidden behind it. The method is designed for the programmable rendering pipelines of modern graphics hardware and is capable of displaying complex models in real-time. An extension of this newly developed illustrative display style was also applied to augmented reality video streams. This system constitutes another realization of the concept of stylized augmented reality.",
            "collectionName":"Arbeit",
            "facets":{
                "provider":"mendeley",
                "year":"2006"
            }
        },
        {
            "id":"00edb47c-a449-32f9-8f99-5c3ab275ec96",
            "title":"A stereoscopic video see-through augmented reality system based on real-time vision-based registration",
            "uri":"http://www.mendeley.com/catalog/stereoscopic-video-seethrough-augmented-reality-system-based-onrealtime-visionbased-registration/",
            "eexcessURI":"http://www.mendeley.com/catalog/stereoscopic-video-seethrough-augmented-reality-system-based-onrealtime-visionbased-registration/",
            "creator":"M. Kanbara, T. Okuma, H. Takemura, N. Yokoya",
            "description":"In an augmented reality system, it is required to obtain the position and orientation of the user's viewpoint in order to display the composed image while maintaining a correct registration between the real and virtual worlds. All the procedures must be done in real time. This paper proposes a method for augmented reality with a stereo vision sensor and a video see-through head-mounted display (HMD). It can synchronize the display timing between the virtual and real worlds so that the alignment error is reduced. The method calculates camera parameters from three markers in image sequences captured by a pair of stereo cameras mounted on the HMD. In addition, it estimates the real-world depth from a pair of stereo images in order to generate a composed image maintaining consistent occlusions between real and virtual objects. The depth estimation region is efficiently limited by calculating the position of the virtual object by using the camera parameters. Finally, we have developed a video see-through augmented reality system which mainly consists of a pair of stereo cameras mounted on the HMD and a standard graphics workstation. The feasibility of the system has been successfully demonstrated with experiments",
            "collectionName":"Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)",
            "facets":{
                "provider":"mendeley",
                "year":"2000"
            }
        },
        {
            "id":"1b285429-85d2-3b55-b738-4b2083dbde9a",
            "title":"Augmented reality",
            "uri":"http://www.mendeley.com/research/augmented-reality-144/",
            "eexcessURI":"http://www.mendeley.com/research/augmented-reality-144/",
            "creator":" Wikipedia contributors",
            "description":"Augmented reality (AR) is a live, direct or indirect, view of a physical, real-world environment whose elements are augmented by computer-generated sensory input such as sound, video, graphics or GPS data. It is related to a more general concept called mediated reality, in which a view of reality is modified (possibly even diminished rather than augmented) by a computer. As a result, the technology functions by enhancing one&amp;rsquo;s current perception of reality.[1] By contrast, virtual reality replaces the real world with a simulated one.[2][3] Augmentation is conventionally in real-time and in semantic context with environmental elements, such as sports scores on TV during a match. With the help of advanced AR technology (e.g. adding computer vision and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulable. Artificial information about the environment and its objects can be overlaid on the real world.",
            "collectionName":"Wikipedia",
            "facets":{
                "provider":"mendeley",
                "year":"2013"
            }
        },
        {
            "id":"5fc4264a-809a-3368-9e99-883c3b82e7f1",
            "title":"The World through the Computer: Computer Augmented Interaction with Real World Environments",
            "uri":"http://www.mendeley.com/catalog/world-through-computer-computer-augmented-interaction-real-world-environments/",
            "eexcessURI":"http://www.mendeley.com/catalog/world-through-computer-computer-augmented-interaction-real-world-environments/",
            "creator":"Jun Rekimoto, Katashi Nagao",
            "description":"Current user interface techniques such as WIMP or the desk- top metaphor do not support real world tasks, because the focus of these user interfaces is only on humancomputer in- teractions, not on humanreal world interactions. In this pa- per, we propose a method of building computer augmented environments using a situation-aware portable device. This device, called NaviCam, has the ability to recognize the users situation by detecting color-code IDs in real world environ- ments. It displays situation sensitive information by superim- posing messages on its video see-through screen. Combina- tion of ID-awareness and portable video-see-through display solves several problems with current ubiquitous computers systems and augmented reality systems.",
            "collectionName":"Proc 8th Ann ACM Symp User Interface and Software Technology UIST ACM Press",
            "facets":{
                "provider":"mendeley",
                "year":"1995"
            }
        },
        {
            "id":"dc723497-17e2-3f60-9b7f-81ebb718ae70",
            "title":"Real-time vision-based camera tracking for augmented reality applications",
            "uri":"http://www.mendeley.com/catalog/realtime-visionbased-camera-tracking-augmented-reality-applications/",
            "eexcessURI":"http://www.mendeley.com/catalog/realtime-visionbased-camera-tracking-augmented-reality-applications/",
            "creator":"Dieter Koller, Gudrun Klinker, Eric Rose, David Breen, Ross Whitaker, Mihran Tuceryan",
            "description":"Augmented reality deals with the problem of dynamically augmenting or enhancing (images or live video of) the real world with computer generated data (e.g., graphics of virtual objects). This poses two major problems: (a) determining the precise alignment of real and virtual coordinate frames for overlay, and (b) capturing the 3D environment including camera and object motions. The latter is important for interactive augmented reality applications where users can interact with both real and virtual objects. Here we address the problem of accurately tracking the 3D motion of a monocular camera in a known 3D environment and dynamically estimating the 3D camera location. We utilize fully automated landmark-based camera calibration to initialize the motion estimation and employ extended Kalman filter techniques to track landmarks and to estimate the camera location. The implementation of our approach has been proven to be efficient and robust and our system successfully tracks in real-time at approximately 10 Hz.",
            "collectionName":"Proceedings of the ACM symposium on Virtual reality software and technology - VRST '97",
            "facets":{
                "provider":"mendeley",
                "year":"1997"
            }
        },
        {
            "id":"d539eee5-fe9a-359b-aaab-20ad8768951e",
            "title":"Handbook of augmented reality",
            "uri":"http://www.mendeley.com/catalog/handbook-augmented-reality-1/",
            "eexcessURI":"http://www.mendeley.com/catalog/handbook-augmented-reality-1/",
            "creator":"Borko Furht",
            "description":"Augmented Reality (AR) refers to a live view of physical real world environment whose elements are merged with augmented computer-generated images creating a mixed reality. The augmentation is typically done in real time and in semantic context with environmental elements. By using the latest AR techniques and technologies, the information about the surrounding real world becomes interactive and digitally usable. The objective of this Handbook is to provide comprehensive guidelines on the current and future trends in augmented reality technologies and applications. This Handbook is carefully edited book contributors are worldwide experts in the field of augmented reality and its applications. The Handbook Advisory Board, comprised of 11 researchers and practitioners from academia and industry, helped in reshaping the Handbook and selecting the right topics and creative and knowledgeable contributors. TheHandbook comprises of two parts,which consist of 33 chapters. The first part on Technologies includes articles dealing with fundamentals of augmented reality, augmented reality technologies, visualization techniques, head-mounted projection displays, evaluation of AR systems, mobile AR systems, and other innovative AR concepts. The second part on Applications includes various articles on AR applications in- cluding applications in psychology,medical education, edutainment, reality games, rehabilitation engineering, automotive safety, product development and manufac- turing, military applications, exhibition and entertainment, geographic information systems, and others. With the dramatic growth of augmented reality and its applications, this Hand- book can be the definitive resource for persons working in this field as researchers, scientists, programmers, engineers, and users. The book is intended for a wide vari- ety of people including academicians, designers, developers, educators, engineers, practitioners, researchers, and graduate students. This book can also be beneficial for business managers, entrepreneurs, and investors. The book can have a great potential to be adopted as a textbook in current and new courses on Augmented Reality. The main features of this Handbook can be summarized as: 1. The Handbook describes and evaluates the current state-of-the-art in the field of augmented reality. 2. The book presents current trends and concepts of augmented reality, technologies and techniques,AR devices, interfaces, tools, and systems applied in AR, as well as current and future applications. 3. Contributors to the Handbook are the leading researchers from academia and practitioners from industry. We would like to thank the authors for their contributions.Without their expertise and effort thisHandbookwould never come to fruition. Springer editors and staff also deserve our sincere recognition for their support throughout the project.",
            "collectionName":"Handbook of Augmented Reality",
            "facets":{
                "provider":"mendeley",
                "year":"2011"
            }
        },
        {
            "id":"4fb2cb6e-b354-3186-9063-f41dbc48b160",
            "title":"Collaborative augmented reality ping-pong via markerless real rackets",
            "uri":"http://www.mendeley.com/catalog/collaborative-augmented-reality-pingpong-via-markerless-real-rackets/",
            "eexcessURI":"http://www.mendeley.com/catalog/collaborative-augmented-reality-pingpong-via-markerless-real-rackets/",
            "creator":"Yong Yan, Xiaowu Chen, Xin Li",
            "description":"This article proposes a method of constructing a ping-pong system via marker less real rackets in collaborative augmented reality. Except a pair of video cameras, without any other sensors or artificial markers, users can use real rackets to hit virtual ping-pong ball on a virtual table and interact with remote partners in augmented reality scene just as they were playing ping-pong in the same place. First, the real racket can be detected and tracked in real-time in the video captured by a single camera in each site. By 3D registration, the real racket can seamlessly interact with the virtual ping-pong ball and table. Then, a communication scheme is designed for the consistent perception between users in collaborative augmented reality ping-pong system. To achieve real-time interaction, the whole method is implemented in a parallel computing environment through multi-core processors. Experimental results demonstrate that our system can provide consistent perception and natural user interaction with low latency and high precision.",
            "collectionName":"Proceedings - 2011 International Conference on Virtual Reality and Visualization, ICVRV 2011",
            "facets":{
                "provider":"mendeley",
                "year":"2011"
            }
        },
        {
            "id":"8dc07664-eaab-3b7b-804f-3832467ff1af",
            "title":"An image overlay system for medical data visualization.",
            "uri":"http://www.mendeley.com/catalog/image-overlay-system-medical-data-visualization/",
            "eexcessURI":"http://www.mendeley.com/catalog/image-overlay-system-medical-data-visualization/",
            "creator":"M Blackwell, C Nikou, A M DiGioia, T Kanade",
            "description":"Image Overlay is a computer display technique which superimposes computer images over the user's direct view of the real world. The images are transformed in real-time so they appear to the user to be an integral part of the surrounding environment. By using Image Overlay with three-dimensional medical images such as CT reconstructions, a surgeon can visualize the data 'in-vivo', exactly positioned within the patient's anatomy, and potentially enhance the surgeon's ability to perform a complex procedure. This paper describes prototype Image Overlay systems and initial experimental results from those systems.",
            "collectionName":"Medical image analysis",
            "facets":{
                "provider":"mendeley",
                "year":"2000"
            }
        },
        {
            "id":"7202be3f-140d-3ca6-86b0-da55efb00769",
            "title":"PixMix: A real-time approach to high-quality Diminished Reality",
            "uri":"http://www.mendeley.com/research/pixmix-realtime-approach-highquality-diminished-reality/",
            "eexcessURI":"http://www.mendeley.com/research/pixmix-realtime-approach-highquality-diminished-reality/",
            "creator":"Jan Herling, Wolfgang Broll",
            "description":"Diminished Reality (DR) allows to remove objects from a video stream while preseving a frame to frame coherence. Some approaches apply a pseudo-DR, allowing for the removal of objects only, while their background can be observed by a second camera. Most real DR approaches are highly computational expensive, not even allowing for interactive rates and/or apply significant restrictions regarding the uniformity of the background, or allow linear camera movements or even a static camera only. In this paper we will present a real-time capable Diminished Reality approach for high-quality image manipulation. Our approach achieves a significantly better performance and image quality for almost planar but non-trivial image backgrounds. Our Diminished Reality pipeline provides coherent video streams even for nonlinear camera movements due to the integration of homography based object tracking.",
            "collectionName":"ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers",
            "facets":{
                "provider":"mendeley",
                "year":"2012"
            }
        },
        {
            "id":"eac86e25-8c02-363b-84bd-c4a9d4072cc2",
            "title":"EyeTap Devices for Augmented, Deliberately Diminished, or Otherwise Altered Visual Perception of Rigid Planar Patches of Real-World Scenes",
            "uri":"http://www.mendeley.com/catalog/eyetap-devices-augmented-deliberately-diminished-otherwise-altered-visual-perception-rigid-planar-pa/",
            "eexcessURI":"http://www.mendeley.com/catalog/eyetap-devices-augmented-deliberately-diminished-otherwise-altered-visual-perception-rigid-planar-pa/",
            "creator":"Steve Mann, James Fung",
            "description":"Diminished reality is as important as augmented reality, and both are possible with a device called the Reality Mediator. Over the past two decades, we have designed, built, worn, and tested many different embodiments of this device in the context of wearable computing. Incorporated into the Reality Mediator is an &ldquo;EyeTap&rdquo; system, which is a device that quantifies and resynthesizes light that would otherwise pass through one or both lenses of the eye(s) of a wearer. The functional principles of EyeTap devices are discussed, in detail. The EyeTap diverts into a spatial measurement system at least a portion of light that would otherwise pass through the center of projection of at least one lens of an eye of a wearer. The Reality Mediator has at least one mode of operation in which it reconstructs these rays of light, under the control of a wearable computer system. The computer system then uses new results in algebraic projective geometry and comparametric equations to perform head tracking, as well as to track motion of rigid planar patches present in the scene. We describe how our tracking algorithm allows an EyeTap to alter the light from a particular portion of the scene to give rise to a computer-controlled, selectively mediated reality. An important difference between mediated reality and augmented reality includes the ability to not just augment but also deliberately diminish or otherwise alter the visual perception of reality. For example, diminished reality allows additional information to be inserted without causing the user to experience information overload. Our tracking algorithm also takes into account the effects of automatic gain control, by performing motion estimation in both spatial as well as tonal motion coordinates.",
            "collectionName":"Teleoperators and Virtual Environments",
            "facets":{
                "provider":"mendeley",
                "year":"2002"
            }
        },
        {
            "id":"e48aa505-f3b0-37c1-b02a-19860456ff83",
            "title":"Multiview paraperspective projection model for diminished reality",
            "uri":"http://www.mendeley.com/catalog/multiview-paraperspective-projection-model-diminished-reality/",
            "eexcessURI":"http://www.mendeley.com/catalog/multiview-paraperspective-projection-model-diminished-reality/",
            "creator":"S. Zokai, J. Esteve, Y. Genc, N. Navab",
            "description":" This paper introduces a \"diminished reality\" technique for removing an object or collection of objects and replacing it with an appropriate background image. Diminished reality can be considered an important part of many mixed and augmented reality applications. Our target application is the use of augmented reality (AR) to revamp procedures in industrial plants. An object or a region of interest is delineated on a single reference image. A paraperspective projection model is used to generate the correct background from multiple calibrated views of the scene. We propose methods to deal with approximately planar backgrounds with different orientations. We also propose a multi-resolution approach to deal with non-planar backgrounds. Different sets of experimental results demonstrate the success and limits of the algorithms. Results on real data from water treatment and power plants show the usefulness of this method for industrial applications.",
            "collectionName":"The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.",
            "facets":{
                "provider":"mendeley",
                "year":"2003"
            }
        },
        {
            "id":"2185247d-3de8-3176-ac87-c169249a9aaf",
            "title":"   Advanced self-contained object removal for realizing real-time diminished reality in unconstrained environments",
            "uri":"http://www.mendeley.com/research/advanced-selfcontained-object-removal-realizing-realtime-diminished-reality-unconstrained-environmen/",
            "eexcessURI":"http://www.mendeley.com/research/advanced-selfcontained-object-removal-realizing-realtime-diminished-reality-unconstrained-environmen/",
            "creator":"Jan Herling, Wolfgang Broll",
            "description":"While Augmented Reality has always been restricted to adding artificial content to the real environment, Diminished Reality allows for removing real world content. Existing approaches however, either require complex setups or are not applicable in real-time. In this paper we present our approach for removing real-world objects from a live video stream of the user's real environment. Our approach is based on a simple setup and neither requires any pre-processing nor any information on the structure and location of the objects to be removed or on their background. Our approach is based on the identification of the objects to be removed combined with an image completion and synthesis algorithm. The performance of our approach is one to two magnitudes better than that of previous work in the area of image completion, providing real-time object cancellation on standard laptop or tablet computers.",
            "collectionName":"9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings",
            "facets":{
                "provider":"mendeley",
                "year":"2010"
            }
        },
        {
            "id":"02a0c2ca-6e6e-375d-a035-cb973df58cb0",
            "title":"Robust object recognition with cortex-like mechanisms",
            "uri":"http://www.mendeley.com/catalog/robust-object-recognition-cortexlike-mechanisms/",
            "eexcessURI":"http://www.mendeley.com/catalog/robust-object-recognition-cortexlike-mechanisms/",
            "creator":"Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, Tomaso Poggio",
            "description":"We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.",
            "collectionName":"IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "facets":{
                "provider":"mendeley",
                "year":"2007"
            }
        },
        {
            "id":"51166ea6-e3e1-3687-978e-1dfcaa4e5834",
            "title":"Contextual priming for object detection",
            "uri":"http://www.mendeley.com/catalog/contextual-priming-object-detection/",
            "eexcessURI":"http://www.mendeley.com/catalog/contextual-priming-object-detection/",
            "creator":"Antonio Torralba",
            "description":"There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",
            "collectionName":"International Journal of Computer Vision",
            "facets":{
                "provider":"mendeley",
                "year":"2003"
            }
        },
        {
            "id":"6e1e4ab3-8034-395f-832f-6a3b1ce0a3ed",
            "title":"Marker tracking and HMD calibration for a video-based augmented reality conferencing system",
            "uri":"http://www.mendeley.com/catalog/marker-tracking-hmd-calibration-videobased-augmentedreality-conferencing-system/",
            "eexcessURI":"http://www.mendeley.com/catalog/marker-tracking-hmd-calibration-videobased-augmentedreality-conferencing-system/",
            "creator":"H. Kato, M. Billinghurst",
            "description":"We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display (HMD) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking",
            "collectionName":"Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)",
            "facets":{
                "provider":"mendeley",
                "year":"1999"
            }
        },
        {
            "id":"5a19dba6-bf97-3294-9618-343f79f1c71f",
            "title":"EXMAR: EXpanded view of mobile augmented reality",
            "uri":"http://www.mendeley.com/catalog/exmar-expanded-view-mobile-augmented-reality/",
            "eexcessURI":"http://www.mendeley.com/catalog/exmar-expanded-view-mobile-augmented-reality/",
            "creator":"Sungjae Hwang, Hyungeun Jo, Jung Hee Ryu",
            "description":"There have been many studies to minimize the psychological and physical load increase caused by mobile augmented reality systems. In this paper, we propose a new technique called &amp;amp;amp;#x201C;EXMAR&amp;amp;amp;#x201D;, which enables the user to explore his/her surroundings with an expanded field of view, resulting in a decrease of physical movement. Through this novel interaction technique, the user can explore off-screen point of interests with environmental contextual information by simple dragging gestures. To evaluate this initial approach, we conducted a proof of concept usability test under a set of scenarios such as &amp;amp;amp;#x201C;Exploring objects behind the user&amp;amp;amp;#x201D;, &amp;amp;amp;#x201C;Avoiding the invasion of personal space&amp;amp;amp;#x201D; and &amp;amp;amp;#x201C;Walk and type with front-view.&amp;amp;amp;#x201D; Through this initial examination, we found that users can explore off-screen point of interests and grasp the spatial relations without the increase of mental effort. We believe that this preliminary study gives a meaningful indication that employing the interactive field of view can be a useful method to decrease the physical load without any additional mental efforts in a mixed and augmented reality environment.",
            "collectionName":"9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings",
            "facets":{
                "provider":"mendeley",
                "year":"2010"
            }
        },
        {
            "id":"432c7359-3645-3296-b0cb-a7eade1daf8f",
            "title":"Explorations in the Use of Augmented Reality for Geographic Visualization",
            "uri":"http://www.mendeley.com/catalog/explorations-augmented-reality-geographic-visualization/",
            "eexcessURI":"http://www.mendeley.com/catalog/explorations-augmented-reality-geographic-visualization/",
            "creator":"Nicholas R. Hedley, Mark Billinghurst, Lori Postner, Richard May, Hirokazu Kato",
            "description":"In this paper, we describe two explorations in the use of hybrid user interfaces for collaborative geographic data visualization. Our first interface combines three technologies: augmented reality (AR), immersive virtual reality (VR), and computer vision-based hand and object tracking. Wearing a lightweight display with an attached camera, users can look at a real map and see three-dimensional virtual terrain models overlaid on the map. From this AR interface, they can fly in and experience the model immersively, or use free hand gestures or physical markers to change the data representation. Building on this work, our second interface explores alternative interface techniques, including a zoomable user interface, paddle interactions, and pen annotations. We describe the system hardware and software and the implications for GIS and spatial science applications.",
            "collectionName":"Presence: Teleoperators and Virtual Environments",
            "facets":{
                "provider":"mendeley",
                "year":"2002"
            }
        },
        {
            "id":"571961d8-e0e0-3267-95a0-7784dfe55f76",
            "title":"A survey of augmented reality",
            "uri":"http://www.mendeley.com/catalog/survey-augmented-reality/",
            "eexcessURI":"http://www.mendeley.com/catalog/survey-augmented-reality/",
            "creator":"Ronald Azuma, Ronald Azuma",
            "description":"This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the  medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of  Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas  requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.",
            "collectionName":"Presence: Teleoperators and Virtual Environments",
            "facets":{
                "provider":"mendeley",
                "year":"1997"
            }
        },
        {
            "id":"c6bd683b-cf1f-398e-80c4-43f53629a049",
            "title":"Simulated augmented reality windshield display as a cognitive mapping aid for elder driver navigation",
            "uri":"http://www.mendeley.com/catalog/simulated-augmented-reality-windshield-display-cognitive-mapping-aid-elder-driver-navigation/",
            "eexcessURI":"http://www.mendeley.com/catalog/simulated-augmented-reality-windshield-display-cognitive-mapping-aid-elder-driver-navigation/",
            "creator":"SeungJun Kim, SeungJun Kim, Anind K. Dey, Anind K. Dey",
            "description":"A common effect of aging is decline in spatial cognition. This is an issue for all elders, but particularly for elder drivers. To address this driving issue, we propose a novel concept of an in-vehicle navigation display system that displays navigation information directly onto the vehicle's windshield, superimposing it on the driver's view of the actual road. An evaluation of our simulated version of this display shows that it results in a significant reduction in navigation errors and distraction-related measures compared to a typical in-car navigation display for elder drivers. These results help us understand how context-sensitive information and a simulated augmented reality representation can be combined to minimize the cognitive load in translating between virtual/information spaces and the real world.",
            "collectionName":"CHI",
            "facets":{
                "provider":"mendeley",
                "year":"2009"
            }
        },
        {
            "id":"67980657-cb39-3e06-9dcf-aab89ae16df9",
            "title":"Zooming Interfaces for Augmented Reality Browsers",
            "uri":"http://www.mendeley.com/catalog/zooming-interfaces-augmented-reality-browsers/",
            "eexcessURI":"http://www.mendeley.com/catalog/zooming-interfaces-augmented-reality-browsers/",
            "creator":"Mulloni Alessandro, Andreas D&amp;uuml;nser, Dieter Schmalstieg",
            "description":"Augmented Reality combines real world and virtual information in interactive visualizations. Since phones started integrating GPS, compass and accelerometer, several Augmented Reality browsers for phones have hit the market. These are applications that access large amounts of geo-referenced information from online sources and present it at corresponding physical locations, superimposed onto a live video stream. However, Augmented Reality is constrained by the camera's field of view and restricted to first- person views, limiting the amount of overview that users can gain. We present two zooming interfaces that compensate for these constraints by enabling users to smoothly zoom between the Augmented Reality view and (1) an egocentric panoramic view of 360, and (2) an exocentric top-down view. We present the results from two studies that show how in most search tasks our zooming interfaces are faster and require less panning than an overlay- based tool, scaling better as the amount of information grows.",
            "collectionName":"Information Systems Journal",
            "facets":{
                "provider":"mendeley",
                "year":"2010"
            }
        }
    ]
}
